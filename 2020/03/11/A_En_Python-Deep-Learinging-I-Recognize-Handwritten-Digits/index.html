<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><title>Neural Networks Learning I -Recognize Handwritten Digits | KnifeLee's Personal Blog</title><meta name="description" content="Neural Networks Learning I -Recognize Handwritten Digits"><meta name="keywords" content="Python,DeepLearning"><meta name="author" content="Knifelee"><meta name="copyright" content="Knifelee"><meta name="format-detection" content="telephone=no"><link rel="shortcut icon" href="/img/favicon.ico"><link rel="preconnect" href="//cdn.jsdelivr.net"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:title" content="Neural Networks Learning I -Recognize Handwritten Digits"><meta name="twitter:description" content="Neural Networks Learning I -Recognize Handwritten Digits"><meta name="twitter:image" content="http://michaelnielsen.org/images/mn.jpg"><meta property="og:type" content="article"><meta property="og:title" content="Neural Networks Learning I -Recognize Handwritten Digits"><meta property="og:url" content="https://knifelees3.github.io/2020/03/11/A_En_Python-Deep-Learinging-I-Recognize-Handwritten-Digits/"><meta property="og:site_name" content="KnifeLee's Personal Blog"><meta property="og:description" content="Neural Networks Learning I -Recognize Handwritten Digits"><meta property="og:image" content="http://michaelnielsen.org/images/mn.jpg"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script src="https://cdn.jsdelivr.net/npm/js-cookie/dist/js.cookie.min.js"></script><script>const autoChangeMode = 'false'
var t = Cookies.get("theme");
if (autoChangeMode == '1'){
const isDarkMode = window.matchMedia("(prefers-color-scheme: dark)").matches
const isLightMode = window.matchMedia("(prefers-color-scheme: light)").matches
const isNotSpecified = window.matchMedia("(prefers-color-scheme: no-preference)").matches
const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

if (t === undefined){
  if (isLightMode) activateLightMode()
  else if (isDarkMode) activateDarkMode()
  else if (isNotSpecified || hasNoSupport){
    console.log('You specified no preference for a color scheme or your browser does not support it. I Schedule dark mode during night time.')
    now = new Date();
    hour = now.getHours();
    isNight = hour < 6 || hour >= 18
    isNight ? activateDarkMode() : activateLightMode()
}
} else if (t == 'light') activateLightMode()
else activateDarkMode()


} else if (autoChangeMode == '2'){
  now = new Date();
  hour = now.getHours();
  isNight = hour < 6 || hour >= 18
  if(t === undefined) isNight? activateDarkMode() : activateLightMode()
  else if (t === 'light') activateLightMode()
  else activateDarkMode() 
} else {
  if ( t == 'dark' ) activateDarkMode()
  else if ( t == 'light') activateLightMode()
}

function activateDarkMode(){
  document.documentElement.setAttribute('data-theme', 'dark')
  if (document.querySelector('meta[name="theme-color"]') !== null){
    document.querySelector('meta[name="theme-color"]').setAttribute('content','#000')
  }
}
function activateLightMode(){
  document.documentElement.setAttribute('data-theme', 'light')
  if (document.querySelector('meta[name="theme-color"]') !== null){
  document.querySelector('meta[name="theme-color"]').setAttribute('content','#fff')
  }
}</script><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><link rel="canonical" href="https://knifelees3.github.io/2020/03/11/A_En_Python-Deep-Learinging-I-Recognize-Handwritten-Digits/"><link rel="prev" title="写在疫情清零之后" href="/https:/knifelees3.github.io/2020/03/20/B_%E9%9A%8F%E7%AC%94_%E5%86%99%E5%9C%A8%E7%96%AB%E6%83%85%E6%B8%85%E9%9B%B6%E4%B9%8B%E5%90%8E/"><link rel="next" title="Typora 使用技巧" href="/https:/knifelees3.github.io/2020/03/02/C_%E6%95%99%E7%A8%8B-Typora%E4%BD%BF%E7%94%A8%E6%8A%80%E5%B7%A7/"><link rel="dns-prefetch" href="https://hm.baidu.com"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?7c4710045c846acddf6f6813e3d1fa3e";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"We didn't find any results for the search: ${query}"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"cookieDomain":"https://xxx/","msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"简"},
  highlight_copy: 'true',
  highlight_lang: 'false',
  highlight_shrink: 'false',
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  bookmark: {
    title: 'Snackbar.bookmark.title',
    message_prev: 'Press',
    message_next: 'to bookmark this page'
  },
  runtime_unit: 'days',
  copyright: undefined,
  copy_copyright_js: false,
  ClickShowText: {"text":"富强,民主,文明,和谐,自由,平等,公正,法治,爱国,敬业,诚信,友善","fontSize":"15px"},
  medium_zoom: 'false',
  Snackbar: undefined
  
}</script></head><body><div id="header"> <div id="page-header"><span class="pull_left" id="blog_name"><a class="blog_title" id="site-name" href="/">KnifeLee's Personal Blog</a></span><i class="fa fa-bars fa-fw toggle-menu pull_right close" aria-hidden="true"></i><span class="pull_right menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> About</span></a></div></div></span><span class="pull_right" id="search_button"><a class="site-page social-icon search"><i class="fa fa-search fa-fw"></i><span> Search</span></a></span></div></div><div id="mobile-sidebar"><div id="menu_mask"></div><div id="mobile-sidebar-menus"><div class="mobile_author_icon"><img class="lazyload avatar_img" src="/img/avatar.jpg" onerror="onerror=null;src='/img/friend_404.gif'"></div><div class="mobile_post_data"><div class="mobile_data_item is_center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">Articles</div><div class="length_num">78</div></a></div></div><div class="mobile_data_item is_center">      <div class="mobile_data_link"><a href="/tags/"><div class="headline">Tags</div><div class="length_num">36</div></a></div></div><div class="mobile_data_item is_center">     <div class="mobile_data_link"><a href="/categories/"><div class="headline">Categories</div><div class="length_num">7</div></a></div></div></div><hr><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> About</span></a></div></div></div><div id="mobile-sidebar-toc"><div class="toc_mobile_headline">Catalog</div><ol class="toc_mobile_items"><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#Introduction-of-some-functions"><span class="toc_mobile_items-number">1.</span> <span class="toc_mobile_items-text">Introduction of some functions</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#numpy-random"><span class="toc_mobile_items-number">1.1.</span> <span class="toc_mobile_items-text">numpy.random</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#random-randn"><span class="toc_mobile_items-number">1.1.1.</span> <span class="toc_mobile_items-text">random.randn</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#Random-shuffle"><span class="toc_mobile_items-number">1.1.2.</span> <span class="toc_mobile_items-text">Random.shuffle</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#zip"><span class="toc_mobile_items-number">1.2.</span> <span class="toc_mobile_items-text">zip()</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#Matrix-use"><span class="toc_mobile_items-number">1.3.</span> <span class="toc_mobile_items-text">Matrix use</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#My-understanding-on-the-neural-networks"><span class="toc_mobile_items-number">2.</span> <span class="toc_mobile_items-text">My understanding on the neural networks</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#How-we-map-the-input-to-the-output"><span class="toc_mobile_items-number">2.1.</span> <span class="toc_mobile_items-text">How we map the input to the output</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#How-to-measure-the-quality-of-the-mapping"><span class="toc_mobile_items-number">2.2.</span> <span class="toc_mobile_items-text">How to measure the quality of the mapping?</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#How-to-train-our-neural-networks？"><span class="toc_mobile_items-number">2.3.</span> <span class="toc_mobile_items-text">How to train our neural networks？</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#Explanation-of-the-program"><span class="toc_mobile_items-number">3.</span> <span class="toc_mobile_items-text">Explanation of the program</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#How-to-use"><span class="toc_mobile_items-number">3.1.</span> <span class="toc_mobile_items-text">How to use?</span></a></li></ol></li></ol></div></div><div id="body-wrap"><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true">     </i><div class="auto_open" id="sidebar"><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar">     </div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Introduction-of-some-functions"><span class="toc-number">1.</span> <span class="toc-text">Introduction of some functions</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#numpy-random"><span class="toc-number">1.1.</span> <span class="toc-text">numpy.random</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#random-randn"><span class="toc-number">1.1.1.</span> <span class="toc-text">random.randn</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Random-shuffle"><span class="toc-number">1.1.2.</span> <span class="toc-text">Random.shuffle</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#zip"><span class="toc-number">1.2.</span> <span class="toc-text">zip()</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Matrix-use"><span class="toc-number">1.3.</span> <span class="toc-text">Matrix use</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#My-understanding-on-the-neural-networks"><span class="toc-number">2.</span> <span class="toc-text">My understanding on the neural networks</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#How-we-map-the-input-to-the-output"><span class="toc-number">2.1.</span> <span class="toc-text">How we map the input to the output</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#How-to-measure-the-quality-of-the-mapping"><span class="toc-number">2.2.</span> <span class="toc-text">How to measure the quality of the mapping?</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#How-to-train-our-neural-networks？"><span class="toc-number">2.3.</span> <span class="toc-text">How to train our neural networks？</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Explanation-of-the-program"><span class="toc-number">3.</span> <span class="toc-text">Explanation of the program</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#How-to-use"><span class="toc-number">3.1.</span> <span class="toc-text">How to use?</span></a></li></ol></li></ol></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(http://michaelnielsen.org/images/mn.jpg)"><div id="post-info"><div id="post-title"><div class="posttitle">Neural Networks Learning I -Recognize Handwritten Digits</div></div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> Created 2020-03-11<span class="post-meta__separator">|</span><i class="fa fa-history" aria-hidden="true"></i> Updated 2020-03-12</time><span class="post-meta__separator mobile_hidden">|</span><span class="mobile_hidden"><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/%E6%8A%80%E6%9C%AF/">技术</a><i class="fa fa-angle-right" aria-hidden="true"></i><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/%E6%8A%80%E6%9C%AF/EnglishNotes/">EnglishNotes</a></span><div class="post-meta-wordcount"><i class="fa fa-file-word-o post-meta__icon" aria-hidden="true"></i><span>Word count: </span><span class="word-count">2.8k</span><span class="post-meta__separator">|</span><i class="fa fa-clock-o post-meta__icon" aria-hidden="true"></i><span>Reading time: 17 min</span><span class="post-meta__separator">|</span><i class="fa fa-eye post-meta__icon" aria-hidden="true">       </i><span>Post View: </span><span id="busuanzi_value_page_pv"></span></div></div></div></div><div class="layout layout_post" id="content-inner">   <article id="post"><div class="article-container" id="post-content"><p>This is the beginning of my neural networks learning. I have read the books written by <a href="http://michaelnielsen.org/" target="_blank" rel="noopener">Michael Nielsen</a> for a long time and I think now it’s time to complete the learning examples in first chapter of his book. </p>
<p><img alt data-src="http://michaelnielsen.org/images/mn.jpg" class="lazyload"></p>
<h2 id="Introduction-of-some-functions"><a href="#Introduction-of-some-functions" class="headerlink" title="Introduction of some functions"></a>Introduction of some functions</h2><p>I first want to show the use of some functions in his program example. These uses really surprise me a lot.</p>
<h3 id="numpy-random"><a href="#numpy-random" class="headerlink" title="numpy.random"></a><code>numpy.random</code></h3><p>The first function is the random function from the package numpy. In initialize the matrix, the random function is used.</p>
<h4 id="random-randn"><a href="#random-randn" class="headerlink" title="random.randn"></a><code>random.randn</code></h4><blockquote>
<p>Return a sample (or samples) from the “standard normal” distribution.<br>If positive, int_like or int-convertible arguments are provided, randn generates an array of shape (d0, d1, …, dn), filled with random floats sampled from a univariate “normal” (Gaussian) distribution of mean 0 and variance 1 (if any of the d_i are floats, they are first converted to integers by truncation). A single float randomly sampled from the distribution is returned if no argument is provided</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">print(np.random.randn(<span class="number">1</span>))</span></pre></td></tr></table></figure>
<pre><code>[0.34894831]
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">print(np.random.randn(<span class="number">3</span>))</span></pre></td></tr></table></figure>
<pre><code>[0.34444932 0.12172097 1.14900238]
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">print(np.random.randn(<span class="number">2</span>,<span class="number">3</span>))</span></pre></td></tr></table></figure>
<pre><code>[[ 0.49635216  0.22762119 -0.68270641]
 [-2.13526944 -0.82040908 -0.79356388]]
</code></pre><h4 id="Random-shuffle"><a href="#Random-shuffle" class="headerlink" title="Random.shuffle"></a><code>Random.shuffle</code></h4><blockquote>
<p>Modify a sequence in-place by shuffling its contents.</p>
<p>This function only shuffles the array along the first axis of a multi-dimensional array. The order of sub-arrays is changed but their contents remains the same.</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">A=np.random.randn(<span class="number">4</span>,<span class="number">4</span>)</span></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">print(A)</span></pre></td></tr></table></figure>
<pre><code>[[ 0.89532715 -2.34406351 -0.47233016 -0.1943856 ]
 [ 0.57509425 -0.84810353  1.11576561  1.33146725]
 [ 0.81883264  2.25208295 -1.52527099 -1.30444846]
 [ 1.94464225  0.29825984 -0.16625868 -0.35876162]]
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">np.random.shuffle(A)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">print(A)</span></pre></td></tr></table></figure>
<pre><code>[[ 1.94464225  0.29825984 -0.16625868 -0.35876162]
 [ 0.57509425 -0.84810353  1.11576561  1.33146725]
 [ 0.81883264  2.25208295 -1.52527099 -1.30444846]
 [ 0.89532715 -2.34406351 -0.47233016 -0.1943856 ]]
</code></pre><h3 id="zip"><a href="#zip" class="headerlink" title="zip()"></a><code>zip()</code></h3><blockquote>
<p>Python’s zip() function creates an iterator that will aggregate elements from two or more iterables. You can use the resulting iterator to quickly and consistently solve common programming problems, like creating dictionaries. In this tutorial, you’ll discover the logic behind the Python zip() function and how you can use it to solve real-world problems.</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">A=[<span class="string">'1'</span>,<span class="string">'2'</span>,<span class="string">'3'</span>]</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">B=[<span class="string">'A'</span>,<span class="string">'B'</span>,<span class="string">'C'</span>]</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">C=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">ABC=zip(A,B,C)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">print(ABC)</span></pre></td></tr></table></figure>
<pre><code>&lt;zip object at 0x000001D6AB168A08&gt;
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">type(ABC)</span></pre></td></tr></table></figure>
<pre><code>zip
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">list(ABC)</span></pre></td></tr></table></figure>
<pre><code>[(&#39;1&#39;, &#39;A&#39;, 1), (&#39;2&#39;, &#39;B&#39;, 2), (&#39;3&#39;, &#39;C&#39;, 3)]
</code></pre><h3 id="Matrix-use"><a href="#Matrix-use" class="headerlink" title="Matrix use"></a>Matrix use</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">A=np.array([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>])</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> l <span class="keyword">in</span> A[<span class="number">1</span>:]:</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">    print(l)</span></pre></td></tr></table></figure>
<pre><code>2
3
4
5
6
7
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> l <span class="keyword">in</span> A[:<span class="number">-1</span>]:</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">    print(l)</span></pre></td></tr></table></figure>
<pre><code>1
2
3
4
5
6
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">sizes=[<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>]</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">W=[np.random.randn(y, x) <span class="keyword">for</span> x, y <span class="keyword">in</span> zip(sizes[:<span class="number">-1</span>], sizes[<span class="number">1</span>:])]</span></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">print(W)</span></pre></td></tr></table></figure>
<pre><code>[array([[-0.53071848, -0.26905161],
       [-0.75696575, -0.57292324],
       [-1.47093334,  0.060232  ]]), array([[ 1.03193319,  0.58177683,  0.78046451],
       [ 0.14132843, -0.90416154, -0.12645047],
       [ 1.90204955, -0.55866015,  0.39481778],
       [-0.11897701,  1.1277029 ,  0.7584795 ]])]
</code></pre><h2 id="My-understanding-on-the-neural-networks"><a href="#My-understanding-on-the-neural-networks" class="headerlink" title="My understanding on the neural networks"></a>My understanding on the neural networks</h2><h3 id="How-we-map-the-input-to-the-output"><a href="#How-we-map-the-input-to-the-output" class="headerlink" title="How we map the input to the output"></a>How we map the input to the output</h3><p>Previously, I have learned the use of some special functions, now it’s time to give a summary of my understanding. I will not list everything since Michael Nieslen gives wonderful descriptions. </p>
<p>In my view, our problem is that we have a input which is usually a one dimensional array, the output is still an array. What we need do is to map the input to the output correctly.</p>
<p>In real life we will can describe and measure the world in different way. The color, the sound, the taste etc. However, </p>
<blockquote>
<p>Anything is a number.</p>
</blockquote>
<p>The properties in real world can all be mapped into a number space. And what happened in real world can be described by the numbers and  number operations. For example, we use coordinate (x,y,z) to describe the position of the some object. </p>
<p>We create the neural networks and it has many layers. From the mathematical view, the input data $V_{in}$ will be processed through different layers with a matrix and Sigmoid transformation.</p>
<script type="math/tex; mode=display">
\boldsymbol{V}^{i+1}=\sigma(\boldsymbol{W}^{i}\cdot \boldsymbol{V}^{i}+\boldsymbol{b})</script><p>Where $V^{i+1}$ is the value in layer $i+1$ and $\boldsymbol{W}^{i}$ is the weight matrix between layer i and layer j. $\boldsymbol{b}$ is a vector. </p>
<p>So the value vectors in different layers will be linked with the the transformation. So this is how we obtain the output from the input.</p>
<p>In summary, we have a neural networks means we have a series of weight matrix and bias vectors. Different weights, bias, number of layers will give different neural networks. Our neural networks are actually a series of matrix, vectors.</p>
<h3 id="How-to-measure-the-quality-of-the-mapping"><a href="#How-to-measure-the-quality-of-the-mapping" class="headerlink" title="How to measure the quality of the mapping?"></a>How to measure the quality of the mapping?</h3><p>To measure the quality of the mapping, we should compare the output data from our neural networks and the actual data. To measure the quality we can for example define the following cost function</p>
<script type="math/tex; mode=display">
C=|\boldsymbol{y}-\boldsymbol{y}_{0}|^2/n</script><p>If the difference between the output and the actual value is smaller, it means our neural networks works better.</p>
<h3 id="How-to-train-our-neural-networks？"><a href="#How-to-train-our-neural-networks？" class="headerlink" title="How to train our neural networks？"></a>How to train our neural networks？</h3><p>A very important step of deep learning is to train our neural networks. Training means we change our weights and bias vector to let our output closer to the actual result.</p>
<p>To realize this, we need give some modifications after each learning. In machine learning, the down hill method is used to optimize our parameters. What we will do is just the same. The difference is that the effect of weights and bias on the output is more complex. We need choose a road in an abstract space to let the cost function decrease just like we go down in an abstract space.</p>
<p>So the partial derivative will be calculated. Some tricks will be used to let the change of the weight function and bias will always let the cost function decrease.</p>
<script type="math/tex; mode=display">
\nabla C=(\frac{\partial C}{\partial v_{1}},\frac{\partial C}{\partial v_{2}}...)</script><script type="math/tex; mode=display">
\Delta C\approx \nabla C \cdot\Delta \nu</script><p>But what’s really exciting about the equation is that it lets us see how to choose $\Delta \nu$ so as to make $\Delta C$ negative. In particular, suppose we choose</p>
<script type="math/tex; mode=display">
\Delta \nu=-\eta \nabla C</script><p>where $\eta$ is a small, positive parameter (known as the learning rate). So $\Delta C\approx -\eta |\nabla C|^2&lt;0$</p>
<p>and the vector should be updated like this</p>
<script type="math/tex; mode=display">
\nu^{\prime}\rightarrow \nu-\eta \nabla C</script><p>However, for a neural networks, further derivation must be done to calculate the derivative of the weight and bias in each layers. Nielsen gives detailed explanation and proof.<a href="http://neuralnetworksanddeeplearning.com/chap2.html" target="_blank" rel="noopener">Back Propagation Method</a></p>
<p>Here is a simple summary,<br><img alt data-src="http://neuralnetworksanddeeplearning.com/images/tikz21.png" class="lazyload"></p>
<p>The backpropagation equations provide us with a way of computing the gradient of the cost function. Let’s explicitly write this out in the form of an algorithm:</p>
<ul>
<li>Input x: Set the corresponding activation a1 for the input layer.</li>
<li>Feedforward: For each l=2,3,…,L compute $z^{l}=w^{l}a^{l-1}+b^{l}$ and $a^{l}=\sigma_{z^{l}}$.</li>
<li>Output error $\delta^{L}$: Compute the vector $\delta^{L}=\nabla_{a}C\cdot \sigma^{\prime}(z^{L})$.</li>
<li>Backpropagate the error: For each l=L−1,L−2,…,2 compute $\delta^{l}=((w^{l+1})^{T}\delta^{l+1}))\cdot \sigma^{\prime}(z^{l})$.</li>
<li>Output: The gradient of the cost function is given by $\frac{\partial C}{\partial b_{j}^{l}}=\delta_{j}^{l}$ and $\frac{\partial C}{\partial w_{jk}^{l}}=a_{k}^{l-1}\delta_{j}^{l}$.</li>
</ul>
<h2 id="Explanation-of-the-program"><a href="#Explanation-of-the-program" class="headerlink" title="Explanation of the program"></a>Explanation of the program</h2><p>Now I will focus on the program and give my own understanding of the function. I will follow the list of the program. The first is import necessary packages.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="string">"""</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="string">network.py</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="string">~~~~~~~~~~</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="string"></span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="string">A module to implement the stochastic gradient descent learning</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"><span class="string">algorithm for a feedforward neural network.  Gradients are calculated</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"><span class="string">using backpropagation.  Note that I have focused on making the code</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"><span class="string">simple, easily readable, and easily modifiable.  It is not optimized,</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"><span class="string">and omits many desirable features.</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line"><span class="string">"""</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line"><span class="comment">#### Libraries</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># Standard library</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line"><span class="comment"># Third-party libraries</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span></pre></td></tr></table></figure>
<p>Then the Sigmoid function and derivation of Sigmoid function will be defined.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="comment">#### Miscellaneous functions</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(z)</span>:</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">    <span class="string">"""The sigmoid function."""</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">return</span> <span class="number">1.0</span>/(<span class="number">1.0</span>+np.exp(-z))</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid_prime</span><span class="params">(z)</span>:</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">    <span class="string">"""Derivative of the sigmoid function."""</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">return</span> sigmoid(z)*(<span class="number">1</span>-sigmoid(z))</span></pre></td></tr></table></figure>
<p>Then a class named network will be defined. And in this class,the <code>_init_</code> function is as follows<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, sizes)</span>:</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">        <span class="string">"""The list ``sizes`` contains the number of neurons in the</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="string">        respective layers of the network.  For example, if the list</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="string">        was [2, 3, 1] then it would be a three-layer network, with the</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="string">        first layer containing 2 neurons, the second layer 3 neurons,</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"><span class="string">        and the third layer 1 neuron.  The biases and weights for the</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"><span class="string">        network are initialized randomly, using a Gaussian</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"><span class="string">        distribution with mean 0, and variance 1.  Note that the first</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"><span class="string">        layer is assumed to be an input layer, and by convention we</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line"><span class="string">        won't set any biases for those neurons, since biases are only</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line"><span class="string">        ever used in computing the outputs from later layers."""</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">        self.num_layers = len(sizes)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">        self.sizes = sizes</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">        self.biases = [np.random.randn(y, <span class="number">1</span>) <span class="keyword">for</span> y <span class="keyword">in</span> sizes[<span class="number">1</span>:]]</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">        self.weights = [np.random.randn(y, x)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line">                        <span class="keyword">for</span> x, y <span class="keyword">in</span> zip(sizes[:<span class="number">-1</span>], sizes[<span class="number">1</span>:])]</span></pre></td></tr></table></figure></p>
<p>Be careful the use of <code>size[:-1],size[1:]</code> which means the matrix removed the last element and last element respectively. The use of zip is also new to me and <code>self.weights</code> is constructed by a series of matrix with different dimensions.</p>
<p>The  feedforward function that which update $a$ from $l$ layer to $l+1$ layer </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">feedforward</span><span class="params">(self, a)</span>:</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">    <span class="string">"""Return the output of the network if ``a`` is input."""</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">for</span> b, w <span class="keyword">in</span> zip(self.biases, self.weights):</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">        a = sigmoid(np.dot(w, a)+b)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">return</span> a</span></pre></td></tr></table></figure>
<p>Here is the SGD main function</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">SGD</span><span class="params">(self, training_data, epochs, mini_batch_size, eta,</span></span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="params">            test_data=None)</span>:</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">        <span class="string">"""Train the neural network using mini-batch stochastic</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="string">        gradient descent.  The ``training_data`` is a list of tuples</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="string">        ``(x, y)`` representing the training inputs and the desired</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line"><span class="string">        outputs.  The other non-optional parameters are</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"><span class="string">        self-explanatory.  If ``test_data`` is provided then the</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"><span class="string">        network will be evaluated against the test data after each</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"><span class="string">        epoch, and partial progress printed out.  This is useful for</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line"><span class="string">        tracking progress, but slows things down substantially."""</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">if</span> test_data: n_test = len(test_data)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">        n = len(training_data)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> xrange(epochs):</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">            random.shuffle(training_data)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">            mini_batches = [</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line">                training_data[k:k+mini_batch_size]</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line">                <span class="keyword">for</span> k <span class="keyword">in</span> xrange(<span class="number">0</span>, n, mini_batch_size)]</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line">            <span class="keyword">for</span> mini_batch <span class="keyword">in</span> mini_batches:</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line">                self.update_mini_batch(mini_batch, eta)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line">            <span class="keyword">if</span> test_data:</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">21</span></pre></td><td class="code"><pre><span class="line">                <span class="keyword">print</span> <span class="string">"Epoch &#123;0&#125;: &#123;1&#125; / &#123;2&#125;"</span>.format(</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">22</span></pre></td><td class="code"><pre><span class="line">                    j, self.evaluate(test_data), n_test)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">23</span></pre></td><td class="code"><pre><span class="line">            <span class="keyword">else</span>:</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">24</span></pre></td><td class="code"><pre><span class="line">                <span class="keyword">print</span> <span class="string">"Epoch &#123;0&#125; complete"</span>.format(j)</span></pre></td></tr></table></figure>
<p>The main training function. A test data will be used if needed. We give the training data which will be loaded use well defined function</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> mnist_loader</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>training_data, validation_data, test_data = \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="meta">... </span>mnist_loader.load_data_wrapper()</span></pre></td></tr></table></figure>
<p>This function will divide the step of training and show the progress and quality of the neural networks if we give the test data. The function <code>update_mini_batch</code> will update the weights and bias for a given training data.</p>
<p><code>update_mini_batch</code> function is defined as follows</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_mini_batch</span><span class="params">(self, mini_batch, eta)</span>:</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">        <span class="string">"""Update the network's weights and biases by applying</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="string">        gradient descent using backpropagation to a single mini batch.</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="string">        The ``mini_batch`` is a list of tuples ``(x, y)``, and ``eta``</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="string">        is the learning rate."""</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">        nabla_b = [np.zeros(b.shape) <span class="keyword">for</span> b <span class="keyword">in</span> self.biases]</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">        nabla_w = [np.zeros(w.shape) <span class="keyword">for</span> w <span class="keyword">in</span> self.weights]</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">for</span> x, y <span class="keyword">in</span> mini_batch:</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">            delta_nabla_b, delta_nabla_w = self.backprop(x, y)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">            nabla_b = [nb+dnb <span class="keyword">for</span> nb, dnb <span class="keyword">in</span> zip(nabla_b, delta_nabla_b)]</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">            nabla_w = [nw+dnw <span class="keyword">for</span> nw, dnw <span class="keyword">in</span> zip(nabla_w, delta_nabla_w)]</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">        self.weights = [w-(eta/len(mini_batch))*nw</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">                        <span class="keyword">for</span> w, nw <span class="keyword">in</span> zip(self.weights, nabla_w)]</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">        self.biases = [b-(eta/len(mini_batch))*nb</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">                       <span class="keyword">for</span> b, nb <span class="keyword">in</span> zip(self.biases, nabla_b)]</span></pre></td></tr></table></figure>
<p>The partial derivative for weights and biases will be defined first. Then the partial derivative will be calculated use the function <code>backprop</code>. And the new weights will bias will be updated. The most important part is the function <code>backprop</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">backprop</span><span class="params">(self, x, y)</span>:</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">        <span class="string">"""Return a tuple ``(nabla_b, nabla_w)`` representing the</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="string">        gradient for the cost function C_x.  ``nabla_b`` and</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="string">        ``nabla_w`` are layer-by-layer lists of numpy arrays, similar</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="string">        to ``self.biases`` and ``self.weights``."""</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">        nabla_b = [np.zeros(b.shape) <span class="keyword">for</span> b <span class="keyword">in</span> self.biases]</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">        nabla_w = [np.zeros(w.shape) <span class="keyword">for</span> w <span class="keyword">in</span> self.weights]</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">        <span class="comment"># feedforward</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line">        activation = x</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line">        activations = [x] <span class="comment"># list to store all the activations, layer by layer</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">        zs = [] <span class="comment"># list to store all the z vectors, layer by layer</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">for</span> b, w <span class="keyword">in</span> zip(self.biases, self.weights):</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">            z = np.dot(w, activation)+b</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">            zs.append(z)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">            activation = sigmoid(z)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line">            activations.append(activation)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line">        <span class="comment"># backward pass</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line">        delta = self.cost_derivative(activations[<span class="number">-1</span>], y) * \</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line">            sigmoid_prime(zs[<span class="number">-1</span>])</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line">        nabla_b[<span class="number">-1</span>] = delta</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">21</span></pre></td><td class="code"><pre><span class="line">        nabla_w[<span class="number">-1</span>] = np.dot(delta, activations[<span class="number">-2</span>].transpose())</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">22</span></pre></td><td class="code"><pre><span class="line">        <span class="comment"># Note that the variable l in the loop below is used a little</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">23</span></pre></td><td class="code"><pre><span class="line">        <span class="comment"># differently to the notation in Chapter 2 of the book.  Here,</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">24</span></pre></td><td class="code"><pre><span class="line">        <span class="comment"># l = 1 means the last layer of neurons, l = 2 is the</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">25</span></pre></td><td class="code"><pre><span class="line">        <span class="comment"># second-last layer, and so on.  It's a renumbering of the</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">26</span></pre></td><td class="code"><pre><span class="line">        <span class="comment"># scheme in the book, used here to take advantage of the fact</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">27</span></pre></td><td class="code"><pre><span class="line">        <span class="comment"># that Python can use negative indices in lists.</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">28</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">for</span> l <span class="keyword">in</span> xrange(<span class="number">2</span>, self.num_layers):</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">29</span></pre></td><td class="code"><pre><span class="line">            z = zs[-l]</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">30</span></pre></td><td class="code"><pre><span class="line">            sp = sigmoid_prime(z)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">31</span></pre></td><td class="code"><pre><span class="line">            delta = np.dot(self.weights[-l+<span class="number">1</span>].transpose(), delta) * sp</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">32</span></pre></td><td class="code"><pre><span class="line">            nabla_b[-l] = delta</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">33</span></pre></td><td class="code"><pre><span class="line">            nabla_w[-l] = np.dot(delta, activations[-l<span class="number">-1</span>].transpose())</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">34</span></pre></td><td class="code"><pre><span class="line">        <span class="keyword">return</span> (nabla_b, nabla_w)</span></pre></td></tr></table></figure>
<p>This function is a realization of previous explanation of the back propagation methods. And finally, the two other functions</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate</span><span class="params">(self, test_data)</span>:</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">    <span class="string">"""Return the number of test inputs for which the neural</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line"><span class="string">    network outputs the correct result. Note that the neural</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line"><span class="string">    network's output is assumed to be the index of whichever</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"><span class="string">    neuron in the final layer has the highest activation."""</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">    test_results = [(np.argmax(self.feedforward(x)), y)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line">                    <span class="keyword">for</span> (x, y) <span class="keyword">in</span> test_data]</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">return</span> sum(int(x == y) <span class="keyword">for</span> (x, y) <span class="keyword">in</span> test_results)</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cost_derivative</span><span class="params">(self, output_activations, y)</span>:</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line">    <span class="string">"""Return the vector of partial derivatives \partial C_x /</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line"><span class="string">    \partial a for the output activations."""</span></span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">    <span class="keyword">return</span> (output_activations-y)</span></pre></td></tr></table></figure>
<p>which is very easy to understand.</p>
<h3 id="How-to-use"><a href="#How-to-use" class="headerlink" title="How to use?"></a>How to use?</h3><p>This is really a good example of neural networks deep learning. To use this, you can direct download Michael Nielsen’s example. However, he writes this use python2, to use python3, you can use the another example by <a href="https://github.com/MichalDanielDobrzanski/DeepLearningPython35" target="_blank" rel="noopener">MichalDanielDobrzanski</a></p>
<p>after downloading the repository, the file <code>network.py</code> is just like we shown above. The following is the use of the program</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span></pre></td><td class="code"><pre><span class="line">C:\Users\xiail\Documents\Dropbox\Code\Python\Study\Neural-Networks\Study<span class="number">-1</span>\DeepL</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">2</span></pre></td><td class="code"><pre><span class="line">on35 (master -&gt; origin)                                                         </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">3</span></pre></td><td class="code"><pre><span class="line">λ python                                                                        </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">4</span></pre></td><td class="code"><pre><span class="line">Python <span class="number">3.7</span><span class="number">.2</span> (tags/v3<span class="number">.7</span><span class="number">.2</span>:<span class="number">9</span>a3ffc0492, Dec <span class="number">23</span> <span class="number">2018</span>, <span class="number">23</span>:<span class="number">09</span>:<span class="number">28</span>) [MSC v<span class="number">.1916</span> <span class="number">64</span> bit </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">5</span></pre></td><td class="code"><pre><span class="line"> win32                                                                          </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">6</span></pre></td><td class="code"><pre><span class="line">Type <span class="string">"help"</span>, <span class="string">"copyright"</span>, <span class="string">"credits"</span> <span class="keyword">or</span> <span class="string">"license"</span> <span class="keyword">for</span> more information.          </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">7</span></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> mnist_loader                                                         </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">8</span></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>training_data, validation_data, test_data = mnist_loader.load_data_wrapper()</span></pre></td></tr><tr><td class="gutter"><pre><span class="line">9</span></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> network                                                              </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">10</span></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>net = network.Network([<span class="number">784</span>, <span class="number">30</span>, <span class="number">10</span>])                                        </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">11</span></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>net.SGD(training_data, <span class="number">30</span>, <span class="number">10</span>, <span class="number">3.0</span>, test_data=test_data)                    </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">12</span></pre></td><td class="code"><pre><span class="line">Epoch <span class="number">0</span> : <span class="number">8254</span> / <span class="number">10000</span>                                                          </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">13</span></pre></td><td class="code"><pre><span class="line">Epoch <span class="number">1</span> : <span class="number">8367</span> / <span class="number">10000</span>                                                          </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">14</span></pre></td><td class="code"><pre><span class="line">Epoch <span class="number">2</span> : <span class="number">8449</span> / <span class="number">10000</span>                                                          </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">15</span></pre></td><td class="code"><pre><span class="line">Epoch <span class="number">3</span> : <span class="number">8483</span> / <span class="number">10000</span>                                                          </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">16</span></pre></td><td class="code"><pre><span class="line">Epoch <span class="number">4</span> : <span class="number">8517</span> / <span class="number">10000</span>                                                          </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">17</span></pre></td><td class="code"><pre><span class="line">Epoch <span class="number">5</span> : <span class="number">8533</span> / <span class="number">10000</span>                                                          </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">18</span></pre></td><td class="code"><pre><span class="line">Epoch <span class="number">6</span> : <span class="number">8538</span> / <span class="number">10000</span>                                                          </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">19</span></pre></td><td class="code"><pre><span class="line">Epoch <span class="number">7</span> : <span class="number">8541</span> / <span class="number">10000</span>                                                          </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">20</span></pre></td><td class="code"><pre><span class="line">Epoch <span class="number">8</span> : <span class="number">9448</span> / <span class="number">10000</span>                                                          </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">21</span></pre></td><td class="code"><pre><span class="line">Epoch <span class="number">9</span> : <span class="number">9450</span> / <span class="number">10000</span>                                                          </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">22</span></pre></td><td class="code"><pre><span class="line">Epoch <span class="number">10</span> : <span class="number">9446</span> / <span class="number">10000</span>                                                         </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">23</span></pre></td><td class="code"><pre><span class="line">Epoch <span class="number">11</span> : <span class="number">9475</span> / <span class="number">10000</span>                                                         </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">24</span></pre></td><td class="code"><pre><span class="line">Epoch <span class="number">12</span> : <span class="number">9456</span> / <span class="number">10000</span>                                                         </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">25</span></pre></td><td class="code"><pre><span class="line">Epoch <span class="number">13</span> : <span class="number">9473</span> / <span class="number">10000</span>                                                         </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">26</span></pre></td><td class="code"><pre><span class="line">Epoch <span class="number">14</span> : <span class="number">9447</span> / <span class="number">10000</span>                                                         </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">27</span></pre></td><td class="code"><pre><span class="line">Epoch <span class="number">15</span> : <span class="number">9483</span> / <span class="number">10000</span>                                                         </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">28</span></pre></td><td class="code"><pre><span class="line">Epoch <span class="number">16</span> : <span class="number">9501</span> / <span class="number">10000</span>                                                         </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">29</span></pre></td><td class="code"><pre><span class="line">Epoch <span class="number">17</span> : <span class="number">9501</span> / <span class="number">10000</span>                                                         </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">30</span></pre></td><td class="code"><pre><span class="line">Epoch <span class="number">18</span> : <span class="number">9502</span> / <span class="number">10000</span>                                                         </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">31</span></pre></td><td class="code"><pre><span class="line">Epoch <span class="number">19</span> : <span class="number">9501</span> / <span class="number">10000</span>                                                         </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">32</span></pre></td><td class="code"><pre><span class="line">Epoch <span class="number">20</span> : <span class="number">9485</span> / <span class="number">10000</span>                                                         </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">33</span></pre></td><td class="code"><pre><span class="line">Epoch <span class="number">21</span> : <span class="number">9491</span> / <span class="number">10000</span>                                                         </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">34</span></pre></td><td class="code"><pre><span class="line">Epoch <span class="number">22</span> : <span class="number">9519</span> / <span class="number">10000</span>                                                         </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">35</span></pre></td><td class="code"><pre><span class="line">Epoch <span class="number">23</span> : <span class="number">9499</span> / <span class="number">10000</span>                                                         </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">36</span></pre></td><td class="code"><pre><span class="line">Epoch <span class="number">24</span> : <span class="number">9530</span> / <span class="number">10000</span>                                                         </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">37</span></pre></td><td class="code"><pre><span class="line">Epoch <span class="number">25</span> : <span class="number">9504</span> / <span class="number">10000</span>                                                         </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">38</span></pre></td><td class="code"><pre><span class="line">Epoch <span class="number">26</span> : <span class="number">9502</span> / <span class="number">10000</span>                                                         </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">39</span></pre></td><td class="code"><pre><span class="line">Epoch <span class="number">27</span> : <span class="number">9521</span> / <span class="number">10000</span>                                                         </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">40</span></pre></td><td class="code"><pre><span class="line">Epoch <span class="number">28</span> : <span class="number">9506</span> / <span class="number">10000</span>                                                         </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">41</span></pre></td><td class="code"><pre><span class="line">Epoch <span class="number">29</span> : <span class="number">9498</span> / <span class="number">10000</span>                                                         </span></pre></td></tr><tr><td class="gutter"><pre><span class="line">42</span></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt;</span></pre></td></tr></table></figure>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined" target="_blank" rel="noopener">Knifelee</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="/https:/knifelees3.github.io/2020/03/11/A_En_Python-Deep-Learinging-I-Recognize-Handwritten-Digits/">https://knifelees3.github.io/2020/03/11/A_En_Python-Deep-Learinging-I-Recognize-Handwritten-Digits/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noopener">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Python/">Python    </a><a class="post-meta__tags" href="/tags/DeepLearning/">DeepLearning    </a></div><div class="post_share"><div class="social-share" data-image="http://michaelnielsen.org/images/mn.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/js/social-share.min.js"></script></div></div><div class="post-reward"><a class="reward-button"><i class="fa fa-qrcode"></i> Donate<div class="reward-main"><ul class="reward-all"><li class="reward-item"><img class="lazyload post-qr-code__img" src="/img/wechat.jpg"><div class="post-qr-code__desc">微信</div></li><li class="reward-item"><img class="lazyload post-qr-code__img" src="/img/alipay.jpg"><div class="post-qr-code__desc">支付寶</div></li></ul></div></a></div><nav class="pagination_post" id="pagination"><div class="prev-post pull_left"><a href="/2020/03/20/B_%E9%9A%8F%E7%AC%94_%E5%86%99%E5%9C%A8%E7%96%AB%E6%83%85%E6%B8%85%E9%9B%B6%E4%B9%8B%E5%90%8E/"><img class="prev_cover lazyload" data-src="https://raw.githubusercontent.com/knifelees3/my_pictures/master/picgoup/湖北疫情数据.png" onerror="onerror=null;src='/img/404.jpg'"><div class="label">Previous Post</div><div class="prev_info"><span>写在疫情清零之后</span></div></a></div><div class="next-post pull_right"><a href="/2020/03/02/C_%E6%95%99%E7%A8%8B-Typora%E4%BD%BF%E7%94%A8%E6%8A%80%E5%B7%A7/"><img class="next_cover lazyload" data-src="http://typora.io/img/drag-img.gif" onerror="onerror=null;src='/img/404.jpg'"><div class="label">Next Post</div><div class="next_info"><span>Typora 使用技巧</span></div></a></div></nav><div class="relatedPosts"><div class="relatedPosts_headline"><i class="fa fa-fw fa-thumbs-up" aria-hidden="true"></i><span> Recommend</span></div><div class="relatedPosts_list"><div class="relatedPosts_item"><a href="/2021/03/29/C_教程_Gwyddion的安装以及Python包的使用/" title="AFM数据处理软件Gwyddion的安装以及Python包的安装使用"><img class="relatedPosts_cover lazyload"data-src="https://raw.githubusercontent.com/knifelees3/my_pictures/master/picgoup/stacked5.png"><div class="relatedPosts_title">AFM数据处理软件Gwyddion的安装以及Python包的安装使用</div></a></div><div class="relatedPosts_item"><a href="/2020/06/20/A_En_DipoleEmissionNearSphere/" title="Dipole's Emission Near Sphere"><img class="relatedPosts_cover lazyload"data-src="https://raw.githubusercontent.com/knifelees3/my_pictures/master/picgoup/20200107214942441_16601.png"><div class="relatedPosts_title">Dipole's Emission Near Sphere</div></a></div><div class="relatedPosts_item"><a href="/2019/04/17/A_En_MATLAB_ParallelComputing/" title="MATLAB Parallel Computing Summary"><img class="relatedPosts_cover lazyload"data-src="https://raw.githubusercontent.com/knifelees3/my_pictures/master/icons/MATLABICON.png"><div class="relatedPosts_title">MATLAB Parallel Computing Summary</div></a></div><div class="relatedPosts_item"><a href="/2019/07/23/A_En_Python_PlotStudy1_SemiCircle_OscilatorApprox/" title="Python Plot (1) A small ball on a smooth semicircle (Animated)"><img class="relatedPosts_cover lazyload"data-src="https://raw.githubusercontent.com/knifelees3/my_pictures/master/20190723_Oscilator/pi_6.gif"><div class="relatedPosts_title">Python Plot (1) A small ball on a smooth semicircle (Animated)</div></a></div><div class="relatedPosts_item"><a href="/2019/04/17/A_En_Python_ParallelComputing/" title="Python Parallel Computing Summary"><img class="relatedPosts_cover lazyload"data-src="https://raw.githubusercontent.com/knifelees3/my_pictures/master/icons/PythonICON.jpg"><div class="relatedPosts_title">Python Parallel Computing Summary</div></a></div><div class="relatedPosts_item"><a href="/2019/08/04/A_En_Python_PlotStudy2_ThreeOrthogonalVectors/" title="PythonPlot (2) ThreeOrthogonalVectors"><img class="relatedPosts_cover lazyload"data-src="https://raw.githubusercontent.com/knifelees3/my_pictures/master/20190804_pythonplot2_threevectors/1_view1.png"><div class="relatedPosts_title">PythonPlot (2) ThreeOrthogonalVectors</div></a></div></div><div class="clear_both"></div></div><hr><div id="post-comment"><div class="comment_headling"><i class="fa fa-comments fa-fw" aria-hidden="true"></i><span> Comment</span></div><div class="vcomment" id="vcomment"></div><script src="https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js"></script><script>var notify = true == true ? true : false;
var verify = false == true ? true : false;
var GUEST_INFO = ['nick','mail','link'];
var guest_info = 'nick,mail,link'.split(',').filter(function(item){
  return GUEST_INFO.indexOf(item) > -1
});
guest_info = guest_info.length == 0 ? GUEST_INFO :guest_info;

window.valine = new Valine({
  el:'#vcomment',
  notify:notify,
  verify:verify,
  appId:'jR8sBuhmF2OQvpAqh8LWCbt1-gzGzoHsz',
  appKey:'2C5ChxH3zGFNoQ8rO7u9I2Lb',
  placeholder:'you can leave your comment here ~',
  avatar:'monsterid',
  guest_info:guest_info,
  pageSize:'10',
  lang:'en',
  recordIP: true
});</script></div></div></div><footer id="footer" data-type="color"><div id="footer-wrap"><div class="copyright">&copy;2018 - 2021 By Knifelee</div><div class="framework-info"><span>Driven </span><a href="http://hexo.io" target="_blank" rel="noopener"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme </span><a href="https://github.com/jerryc127/hexo-theme-butterfly" target="_blank" rel="noopener"><span>Butterfly</span></a></div><div class="footer_custom_text">Any question please email to me: knifelees3@hotmail.com</div></div></footer></div><section class="rightside" id="rightside"><div id="rightside-config-hide"><i class="fa fa-book" id="readmode" title="Read Mode"></i><i class="fa fa-plus" id="font_plus" title="Increase font size"></i><i class="fa fa-minus" id="font_minus" title="Decrease font size"></i><a class="translate_chn_to_cht" id="translateLink" href="javascript:translatePage();" title="Traditional Chinese and Simplified Chinese Conversion" target="_self">繁</a><i class="darkmode fa fa-moon-o" id="darkmode" title="Dark Mode"></i></div><div id="rightside-config-show"><div id="rightside_config" title="Setting"><i class="fa fa-cog" aria-hidden="true"></i></div><a id="to_comment" href="#post-comment" title="Scroll to comment"><i class="scroll_to_comment fa fa-comments">  </i></a><i class="fa fa-list-ul close" id="mobile-toc-button" title="Table of Contents" aria-hidden="true"></i><i class="fa fa-arrow-up" id="go-up" title="Back to top" aria-hidden="true"></i></div></section><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  CommonHTML: {
    linebreaks: { automatic: true, width: "90% container" }
  },
  "HTML-CSS": { 
    linebreaks: { automatic: true, width: "90% container" }
  },
  "SVG": { 
    linebreaks: { automatic: true, width: "90% container" }
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.jsdelivr.net/npm/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script async src="/js/search/local-search.js"></script><script src="/js/baidupush.js"> </script><script src="/js/tw_cn.js"></script><script>translateInitilization()
</script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page@latest/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/lazysizes@latest/lazysizes.min.js" async=""></script><script src="/js/third-party/ClickShowText.js"></script><div class="search-dialog" id="local-search"><div class="search-dialog__title" id="local-search-title">Local search</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts"></div></div></div><hr><div id="local-search-results"><div id="local-hits"></div><div id="local-stats"><div class="local-search-stats__hr" id="hr"><span>Powered by</span> <a href="https://github.com/wzpan/hexo-generator-search" target="_blank" rel="noopener" style="color:#49B1F5;">hexo-generator-search</a></div></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div></body></html>